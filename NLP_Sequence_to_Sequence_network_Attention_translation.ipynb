{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Park-hanna/BDA_study/blob/main/NLP_Sequence_to_Sequence_network_Attention_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55QdIAUadYg-"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRkCjKp1d9ku"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0:'SOS', 1:'EOS'}\n",
        "    self.n_words = 2\n",
        "\n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxA-l72Se5V7"
      },
      "outputs": [],
      "source": [
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'MN'\n",
        "  )\n",
        "\n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([.!?])\", r\"\\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z.!?]+\",r\" \", s)\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGKGKTyffY4r"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "  print('Reading lines...')\n",
        "\n",
        "  lines = open('/content/drive/MyDrive/Data/data (2)/data/%s-%s.txt'%(lang1, lang2), encoding='utf-8').\\\n",
        "      read().strip().split('\\n')\n",
        "\n",
        "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "  if reverse:\n",
        "    pairs = [list(reversed(p)) for p in pairs]\n",
        "    input_lang = Lang(lang2)\n",
        "    output_lang = Lang(lang1)\n",
        "\n",
        "  else:\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "\n",
        "  return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvwTdC1DgKMa"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "  return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqBNqh8kgy9H",
        "outputId": "18f8fb4c-cec2-45d9-8f88-035fcf23ab89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10760 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4981\n",
            "eng 3653\n",
            "['vous e tes invite es.', 'you re invited.']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse = False):\n",
        "  input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "  print('Read %s sentence pairs'%len(pairs))\n",
        "  pairs = filterPairs(pairs)\n",
        "  print('Trimmed to %s sentence pairs'%len(pairs))\n",
        "  print('Counting words...')\n",
        "  for pair in pairs:\n",
        "    input_lang.addSentence(pair[0])\n",
        "    output_lang.addSentence(pair[1])\n",
        "  print('Counted words:')\n",
        "  print(input_lang.name, input_lang.n_words)\n",
        "  print(output_lang.name, output_lang.n_words)\n",
        "  return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkyPBFTHiVRz"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    embedded = self.embedding(input).view(1,1,-1)\n",
        "    output = embedded\n",
        "    ouput, hidden = self.gru(output, hidden)\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V89NyLH4jTlR"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim =1)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    output = self.embedding(input).view(1,1,-1)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.softmax(self.out(output[0]))\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-nf8-69kKar"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size, dropout_p = 0.1, max_length=MAX_LENGTH):\n",
        "    super(AttnDecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.dropout_p = dropout_p\n",
        "    self.max_length = max_length\n",
        "\n",
        "    self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "    self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "    self.dropout = nn.Dropout(self.dropout_p)\n",
        "    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "    self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "  def forward(self, input, hidden, encoder_outputs):\n",
        "    embedded = self.embedding(input).view(1,1,-1)\n",
        "    embedded = self.dropout(embedded)\n",
        "\n",
        "    attn_weights = F.softmax(\n",
        "        self.attn(torch.cat((embedded[0], hidden[0]),1)), dim=1)\n",
        "    attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                             encoder_outputs.unsqueeze(0))\n",
        "    output = torch.cat((embedded[0], attn_applied[0]),1)\n",
        "    output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "\n",
        "    output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "    return output, hidden, attn_weights\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P71gvne8mPeY"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "  return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "  indexes = indexesFromSentence(lang, sentence)\n",
        "  indexes.append(EOS_token)\n",
        "  return torch.tensor(indexes, dtype = torch.long, device = device).view(-1,1)\n",
        "\n",
        "def tensorFromPair(pair):\n",
        "  input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "  target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "  return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFDKEqGfoYEY"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length = MAX_LENGTH):\n",
        "  encoder_hidden = encoder.initHidden()\n",
        "\n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "\n",
        "  input_length = input_tensor.size(0)\n",
        "  target_length = target_tensor.size(0)\n",
        "\n",
        "  encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  for ei in range(input_length):\n",
        "    encoder_output, encoder_hidden = encoder(\n",
        "        input_tensor[ei], encoder_hidden)\n",
        "    encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "  decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "  decoder_hidden = encoder_hidden\n",
        "\n",
        "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "  if use_teacher_forcing:\n",
        "    for di in range(target_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "          decoder_input, decoder_hidden, encoder_outputs)\n",
        "      loss += criterion(decoder_output, target_tensor[di])\n",
        "      decoder_input = target_tensor[di]\n",
        "\n",
        "  else:\n",
        "    for di in range(target_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "          decoder_input, decoder_hidden, encoder_outputs)\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "      decoder_input = topi.squeeze().detach()\n",
        "\n",
        "      loss += criterion(decoder_output, target_tensor[di])\n",
        "      if decoder_input.item() == EOS_token:\n",
        "        break\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MSYXZU7qgdc"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "  m = math.floor(s/60)\n",
        "  s -= m*60\n",
        "  return '%dm %ds' %(m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "  now = time.time()\n",
        "  s = now-since\n",
        "  es = s/(percent)\n",
        "  rs = es - s\n",
        "  return '%s (- %s)' %(asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvMC57rNq7Yu"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "  start = time.time()\n",
        "  plot_losses = []\n",
        "  print_loss_total = 0\n",
        "  plot_loss_total = 0\n",
        "\n",
        "  encoder_optimizer = optim.SGD(encoder.parameters(), lr = learning_rate)\n",
        "  decoder_optimizer = optim.SGD(decoder.parameters(), lr = learning_rate)\n",
        "  training_pairs = [tensorFromPair(random.choice(pairs))\n",
        "                    for i in range(n_iters)]\n",
        "  criterion = nn.NLLLoss()\n",
        "\n",
        "  for iter in range(1, n_iters + 1):\n",
        "    training_pair = training_pairs[iter - 1]\n",
        "    input_tensor = training_pair[0]\n",
        "    target_tensor = training_pair[1]\n",
        "\n",
        "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "    print_loss_total += loss\n",
        "    plot_loss_total += loss\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "      print_loss_avg = print_loss_total / print_every\n",
        "      print_loss_total = 0\n",
        "      print('%s (%d %d%%) %.4f'%(timeSince(start, iter / n_iters), iter, iter/n_iters * 100, print_loss_avg))\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "      plot_loss_avg = plot_loss_total / plot_every\n",
        "      plot_losses.append(plot_loss_avg)\n",
        "      plot_loss_total = 0\n",
        "\n",
        "\n",
        "  showPlot(plot_losses)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEuilQOFslk7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots()\n",
        "  loc = ticker.MultipleLocator(base = 0.2)\n",
        "  ax.yaxis.set_major_locator(loc)\n",
        "  plt.plot(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOk5f0r2s8X1"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length = MAX_LENGTH):\n",
        "  with torch.no_grad():\n",
        "    input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "    input_length = input_tensor.size()[0]\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    for ei in range(input_length):\n",
        "      encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                               encoder_hidden)\n",
        "      encoder_outputs[ei] += encoder_output[0,0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    decoded_words = []\n",
        "    decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "    for di in range(max_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "          decoder_input, decoder_hidden, encoder_outputs)\n",
        "      decoder_attentions[di] = decoder_attention.data\n",
        "      topv, topi = decoder_output.data.topk(1)\n",
        "      if topi.item() == EOS_token:\n",
        "        decoded_words.append('<EOS>')\n",
        "        break\n",
        "      else:\n",
        "        decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "      decoder_input = topi.squeeze().detach()\n",
        "\n",
        "    return decoded_words, decoder_attentions[:di + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0whi6yZeu9BO"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n = 10):\n",
        "  for i in range(n):\n",
        "    pair = random.choice(pairs)\n",
        "    print('>', pair[0])\n",
        "    print('=', pair[1])\n",
        "    output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    print('<', output_sentence)\n",
        "    print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Hrh0pv5uywo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c98908-e84e-4ed5-c05c-b833cd58471d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 29s (- 20m 57s) (5000 6%) 2.9824\n",
            "2m 54s (- 18m 56s) (10000 13%) 2.5871\n",
            "4m 15s (- 17m 3s) (15000 20%) 2.2790\n",
            "5m 37s (- 15m 27s) (20000 26%) 2.0181\n",
            "6m 58s (- 13m 57s) (25000 33%) 1.8031\n",
            "8m 19s (- 12m 29s) (30000 40%) 1.6238\n",
            "9m 42s (- 11m 5s) (35000 46%) 1.4783\n",
            "11m 4s (- 9m 41s) (40000 53%) 1.3512\n",
            "12m 26s (- 8m 17s) (45000 60%) 1.2125\n",
            "13m 49s (- 6m 54s) (50000 66%) 1.1096\n",
            "15m 12s (- 5m 31s) (55000 73%) 1.0029\n",
            "16m 35s (- 4m 8s) (60000 80%) 0.9148\n",
            "17m 58s (- 2m 45s) (65000 86%) 0.8410\n",
            "19m 22s (- 1m 23s) (70000 93%) 0.8094\n",
            "20m 46s (- 0m 0s) (75000 100%) 0.7993\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rftQh1unvnUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9bf9d9c-ed02-422a-a12c-700593ace028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> vous m intimidez toujours.\n",
            "= i m still intimidated by you.\n",
            "< i m still intimidated by you. <EOS>\n",
            "\n",
            "> je suis italien.\n",
            "= i am italian.\n",
            "< i am italian. <EOS>\n",
            "\n",
            "> je vais bien.\n",
            "= i m doing well.\n",
            "< i m okay. <EOS>\n",
            "\n",
            "> je suis un patient.\n",
            "= i m a patient.\n",
            "< i m a patient. <EOS>\n",
            "\n",
            "> nous nous soucions de ta se curite .\n",
            "= we re concerned for your safety.\n",
            "< we re concerned for your safety. <EOS>\n",
            "\n",
            "> vous n e tes pas fatigue es si ?\n",
            "= you re not tired are you?\n",
            "< you re not married are you? <EOS>\n",
            "\n",
            "> je suis tre s impressionne e.\n",
            "= i m very impressed.\n",
            "< i m very impressed. <EOS>\n",
            "\n",
            "> nous commenc ons a fatiguer.\n",
            "= we re getting tired.\n",
            "< we re getting this of <EOS>\n",
            "\n",
            "> nous e tudions le chinois.\n",
            "= we re studying chinese.\n",
            "< we re studying chinese. <EOS>\n",
            "\n",
            "> je ne suis pas encore pre t.\n",
            "= i m still not ready yet.\n",
            "< i m still not ready yet. <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b-b2CDowQg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef986c3-d00d-4f4b-e6eb-6620bdc0fa5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff88d5a55d0>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, 'je suis trop froid .')\n",
        "plt.matshow(attentions.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjJ31nckwd0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abbd05a4-18ac-4fa6-adca-7e9485e5e40d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = elle a cinq ans de moins que moi .\n",
            "output = she s five years younger than i <EOS>\n",
            "input = elle est trop petit .\n",
            "output = she s too loud. <EOS>\n",
            "input = je ne crains pas de mourir .\n",
            "output = i m not desperate. <EOS>\n",
            "input = c est un jeune directeur plein de talent .\n",
            "output = he is a young young <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-f8657a1772b8>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
            "<ipython-input-63-f8657a1772b8>:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels([''] + output_words)\n"
          ]
        }
      ],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # colorbar로 그림 설정\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # 축 설정\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # 매 틱마다 라벨 보여주기\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1liLGwq2geSCtLAstfyVpltjwxpMGr76Q",
      "authorship_tag": "ABX9TyORlMZeZNztSsuGQQDHzGAp",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}